{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model for Part-of-Speech tagging\n",
    "\n",
    "Part-of-Speech (POS) tagging is a process of labeling words in a text with their respective part-of-speech or word class, e.g., in English language, word classes/lexical categories are such as *nouns*, *verbs*, *adjectives* and *adverbs*. It is an essential step performed in the early part of a NLP pipeline. A well-known purpose of POS tagging is word disambiguition. A word can have different meaning depending its definition and its relationship with other words surrounding it in a sentence. For instance, in the following two sentences, the word \"*book*\" are *noun* and *verb*, respectively. \n",
    "\n",
    "\"*They like the new book.*\" \n",
    "<br>\n",
    "\"*They book a table for dinner.*\" \n",
    "\n",
    "POS tagging also provides features for parsing, coreference resolution and relation extraction. Example of applications are text-to-speech system, translation and etc. \n",
    "\n",
    "In this notebook, we will use the the MASC tagged corpus to train a Hidden Markov Model Tagger. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization\n",
    "#\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import masc_tagged\n",
    "#nltk.download(\"masc_tagged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use MASC tag labeled corpus for supervised training\n",
    "##\n",
    "\n",
    "\n",
    "s_labeled = masc_tagged.tagged_sents()  # List of sentences in word-tag pairs as training data\n",
    "\n",
    "## Split data into 80%-20% as training and test sets\n",
    "#\n",
    "k = round(0.8*len(s_labeled))\n",
    "\n",
    "s_train = [sen for i, sen in enumerate(s_labeled) if i < k]\n",
    "s_test = [sen for i, sen in enumerate(s_labeled) if i >= k]\n",
    "\n",
    "# print('Original : ', len(s_labeled), '\\n',\n",
    "#       'Training : ', len(s_train), '\\n',\n",
    "#       'Testing : ', len(s_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Radio planet text as unlabeled corpus for unsupervised learning  \n",
    "#\n",
    "\n",
    "# with open(r\"C:/Users/meiye/radio_planet_tokens.txt\", encoding='utf8') as fobj:\n",
    "#     L = fobj.readlines()\n",
    "    \n",
    "# s_unlabeled = [word_tokenize(line) for line in L]\n",
    "# s_unlabeled = [[(word, None) for word in sent] for sent in s_unlabeled]\n",
    "\n",
    "\n",
    "# train_unlabeled = [sen for i, sen in enumerate(s_unlabeled) if i < 900]\n",
    "# test_unlabeled = [sen for i, sen in enumerate(s_unlabeled) if i >= 900]\n",
    "\n",
    "# trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maximum Likelihood Estimation vs. Lidstone smoothing\n",
    "#\n",
    "from nltk.util import unique_list\n",
    "from nltk.tag import HiddenMarkovModelTrainer\n",
    "from nltk.probability import *\n",
    "\n",
    "symbols = unique_list(word for sent in s_train for word, tag in sent)\n",
    "tag_set = unique_list(tag for sent in s_train for word, tag in sent)\n",
    "# print('sym_len : ', len(symbols), '\\n', 'tagset_len : ', len(tag_set))\n",
    "\n",
    "\n",
    "## Extend symbols with those in the unlabelled set (for semisupervised learning with additional unlabeled text)\n",
    "# symbols = unique_list(symbols + unique_list(word for sent in train_unlabeled for word, tag in sent))\n",
    "# print('ext_sym_len : ', len(symbols))\n",
    "\n",
    "trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)\n",
    "\n",
    "hmm_mle = trainer.train_supervised(s_train, estimator=lambda fd, bins: MLEProbDist(fd) )\n",
    "hmm_lid00 = trainer.train_supervised(s_train, estimator=lambda fd, bins: LidstoneProbDist(fd, 0, bins))\n",
    "hmm_lid01 = trainer.train_supervised(s_train, estimator=lambda fd, bins: LidstoneProbDist(fd, 0.1, bins))\n",
    "hmm_lid05 = trainer.train_supervised(s_train, estimator=lambda fd, bins: LidstoneProbDist(fd, 0.5, bins))\n",
    "hmm_lid08 = trainer.train_supervised(s_train, estimator=lambda fd, bins: LidstoneProbDist(fd, 0.8, bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE :  0.48726554562370855\n",
      "Lidstone gamma=0 :  0.48726554562370855\n",
      "Lidstone gamma=0.1 :  0.8488076075664692\n",
      "Lidstone gamma=0.5 :  0.8339669166752286\n",
      "Lidstone gamma=0.8 :  0.8240464856102377\n"
     ]
    }
   ],
   "source": [
    "print('MLE : ', hmm_mle.evaluate(s_test))\n",
    "print('Lidstone gamma=0 : ', hmm_lid00.evaluate(s_test))\n",
    "print('Lidstone gamma=0.1 : ', hmm_lid01.evaluate(s_test))\n",
    "print('Lidstone gamma=0.5 : ', hmm_lid05.evaluate(s_test))\n",
    "print('Lidstone gamma=0.8 : ', hmm_lid08.evaluate(s_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
